{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9661299,"sourceType":"datasetVersion","datasetId":5902682},{"sourceId":9667345,"sourceType":"datasetVersion","datasetId":5907181},{"sourceId":9778669,"sourceType":"datasetVersion","datasetId":5990510},{"sourceId":9792121,"sourceType":"datasetVersion","datasetId":5930844}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q wget\n","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:36:26.417619Z","iopub.execute_input":"2024-11-02T05:36:26.418369Z","iopub.status.idle":"2024-11-02T05:36:42.836669Z","shell.execute_reply.started":"2024-11-02T05:36:26.418328Z","shell.execute_reply":"2024-11-02T05:36:42.835205Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd \n\ndata_path = '/kaggle/input/task-3-coling-25/test.csv'\ndata_path = '/kaggle/input/task-3-coling-25/train.csv'\ndf = pd.read_csv(data_path)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-11-14T10:39:16.102388Z","iopub.execute_input":"2024-11-14T10:39:16.102814Z","iopub.status.idle":"2024-11-14T10:44:12.439288Z","shell.execute_reply.started":"2024-11-14T10:39:16.102773Z","shell.execute_reply":"2024-11-14T10:44:12.437232Z"},"trusted":true},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                           id  \\\n0        e5e058ce-be2b-459d-af36-32532aaba5ff   \n1        f95b107b-d176-4af5-90f7-4d0bb20caf93   \n2        856d8972-9e3d-4544-babc-0fe16f21e04d   \n3        fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n4        72c41b8d-0069-4886-b734-a4000ffca286   \n...                                       ...   \n5615815  7446897b-559c-4af7-969d-a76ebbb3b2ea   \n5615816  e7637f9e-0d0d-4db1-ba08-f1a8bb2962e5   \n5615817  7934bf42-8bba-4ead-91d1-6f9a356c386e   \n5615818  e9ff23af-1d49-4224-a4b2-54cccf2ec68b   \n5615819  56426ad1-2fce-4a55-bd4f-a546520a62ff   \n\n                                adv_source_id  \\\n0        e5e058ce-be2b-459d-af36-32532aaba5ff   \n1        f95b107b-d176-4af5-90f7-4d0bb20caf93   \n2        856d8972-9e3d-4544-babc-0fe16f21e04d   \n3        fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n4        72c41b8d-0069-4886-b734-a4000ffca286   \n...                                       ...   \n5615815  dc125710-a42d-4403-94d1-e7e25307a6d1   \n5615816  a0cb4901-551b-4f64-a32b-4b9046bc7209   \n5615817  85fd6cde-1e2b-4c22-a8f7-03121b6f8e8f   \n5615818  4693b180-9160-4daf-9a02-5a64f26d6e45   \n5615819  7be1a55e-694d-47d1-9e9d-e2cfeab8bc36   \n\n                                    source_id     model  decoding  \\\n0        e5e058ce-be2b-459d-af36-32532aaba5ff     human       NaN   \n1        f95b107b-d176-4af5-90f7-4d0bb20caf93     human       NaN   \n2        856d8972-9e3d-4544-babc-0fe16f21e04d     human       NaN   \n3        fbc8a5ea-90fa-47b8-8fa7-73dd954f1524     human       NaN   \n4        72c41b8d-0069-4886-b734-a4000ffca286     human       NaN   \n...                                       ...       ...       ...   \n5615815  b1e69455-73c9-47b6-a849-da38cdef929a  mpt-chat  sampling   \n5615816  c9b8f9f6-8c41-45b0-9509-2466f79d0346  mpt-chat    greedy   \n5615817  c9b8f9f6-8c41-45b0-9509-2466f79d0346  mpt-chat  sampling   \n5615818  defc36fc-661e-400c-a364-03610f90bd04  mpt-chat    greedy   \n5615819  defc36fc-661e-400c-a364-03610f90bd04  mpt-chat  sampling   \n\n        repetition_penalty            attack     domain  \\\n0                      NaN              none  abstracts   \n1                      NaN              none  abstracts   \n2                      NaN              none  abstracts   \n3                      NaN              none  abstracts   \n4                      NaN              none  abstracts   \n...                    ...               ...        ...   \n5615815                 no  zero_width_space       wiki   \n5615816                yes  zero_width_space       wiki   \n5615817                 no  zero_width_space       wiki   \n5615818                yes  zero_width_space       wiki   \n5615819                 no  zero_width_space       wiki   \n\n                                                     title  \\\n0        FUTURE-AI: Guiding Principles and Consensus Re...   \n1        EdgeFlow: Achieving Practical Interactive Segm...   \n2        Semi-supervised Contrastive Learning for Label...   \n3        Combo Loss: Handling Input and Output Imbalanc...   \n4        Attention-Based 3D Seismic Fault Segmentation ...   \n...                                                    ...   \n5615815                                   Preferred number   \n5615816                              Francisco Bethencourt   \n5615817                              Francisco Bethencourt   \n5615818                                         Bart Allen   \n5615819                                         Bart Allen   \n\n                                                    prompt  \\\n0                                                      NaN   \n1                                                      NaN   \n2                                                      NaN   \n3                                                      NaN   \n4                                                      NaN   \n...                                                    ...   \n5615815  Write the body of a Wikipedia article titled \"...   \n5615816  Write the body of a Wikipedia article titled \"...   \n5615817  Write the body of a Wikipedia article titled \"...   \n5615818  Write the body of a Wikipedia article titled \"...   \n5615819  Write the body of a Wikipedia article titled \"...   \n\n                                                generation  \n0        The recent advancements in artificial intellig...  \n1        High-quality training data play a key role in ...  \n2        The success of deep learning methods in medica...  \n3        Simultaneous segmentation of multiple organs f...  \n4        Detection faults in seismic data is a crucial ...  \n...                                                    ...  \n5615815  P​r​e​f​e​r​r​e​d​ ​n​u​m​b​e​r​ ​i​s​ ​a​ ​t​...  \n5615816  F​r​a​n​c​i​s​c​o​ ​d​e​ ​P​a​u​l​a​ ​V​i​c​t​...  \n5615817  F​r​a​n​c​i​s​c​o​ ​B​e​t​h​e​n​c​o​u​r​t​ ​w​...  \n5615818  B​a​r​t​ ​A​l​l​e​n​ ​i​s​ ​a​ ​f​i​c​t​i​o​n​...  \n5615819  B​a​r​t​ ​A​l​l​e​n​ ​i​s​ ​a​ ​f​i​c​t​i​o​n​...  \n\n[5615820 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>adv_source_id</th>\n      <th>source_id</th>\n      <th>model</th>\n      <th>decoding</th>\n      <th>repetition_penalty</th>\n      <th>attack</th>\n      <th>domain</th>\n      <th>title</th>\n      <th>prompt</th>\n      <th>generation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n      <td>human</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>abstracts</td>\n      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n      <td>NaN</td>\n      <td>The recent advancements in artificial intellig...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n      <td>human</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>abstracts</td>\n      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n      <td>NaN</td>\n      <td>High-quality training data play a key role in ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n      <td>human</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>abstracts</td>\n      <td>Semi-supervised Contrastive Learning for Label...</td>\n      <td>NaN</td>\n      <td>The success of deep learning methods in medica...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n      <td>human</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>abstracts</td>\n      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n      <td>NaN</td>\n      <td>Simultaneous segmentation of multiple organs f...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n      <td>human</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>none</td>\n      <td>abstracts</td>\n      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n      <td>NaN</td>\n      <td>Detection faults in seismic data is a crucial ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5615815</th>\n      <td>7446897b-559c-4af7-969d-a76ebbb3b2ea</td>\n      <td>dc125710-a42d-4403-94d1-e7e25307a6d1</td>\n      <td>b1e69455-73c9-47b6-a849-da38cdef929a</td>\n      <td>mpt-chat</td>\n      <td>sampling</td>\n      <td>no</td>\n      <td>zero_width_space</td>\n      <td>wiki</td>\n      <td>Preferred number</td>\n      <td>Write the body of a Wikipedia article titled \"...</td>\n      <td>P​r​e​f​e​r​r​e​d​ ​n​u​m​b​e​r​ ​i​s​ ​a​ ​t​...</td>\n    </tr>\n    <tr>\n      <th>5615816</th>\n      <td>e7637f9e-0d0d-4db1-ba08-f1a8bb2962e5</td>\n      <td>a0cb4901-551b-4f64-a32b-4b9046bc7209</td>\n      <td>c9b8f9f6-8c41-45b0-9509-2466f79d0346</td>\n      <td>mpt-chat</td>\n      <td>greedy</td>\n      <td>yes</td>\n      <td>zero_width_space</td>\n      <td>wiki</td>\n      <td>Francisco Bethencourt</td>\n      <td>Write the body of a Wikipedia article titled \"...</td>\n      <td>F​r​a​n​c​i​s​c​o​ ​d​e​ ​P​a​u​l​a​ ​V​i​c​t​...</td>\n    </tr>\n    <tr>\n      <th>5615817</th>\n      <td>7934bf42-8bba-4ead-91d1-6f9a356c386e</td>\n      <td>85fd6cde-1e2b-4c22-a8f7-03121b6f8e8f</td>\n      <td>c9b8f9f6-8c41-45b0-9509-2466f79d0346</td>\n      <td>mpt-chat</td>\n      <td>sampling</td>\n      <td>no</td>\n      <td>zero_width_space</td>\n      <td>wiki</td>\n      <td>Francisco Bethencourt</td>\n      <td>Write the body of a Wikipedia article titled \"...</td>\n      <td>F​r​a​n​c​i​s​c​o​ ​B​e​t​h​e​n​c​o​u​r​t​ ​w​...</td>\n    </tr>\n    <tr>\n      <th>5615818</th>\n      <td>e9ff23af-1d49-4224-a4b2-54cccf2ec68b</td>\n      <td>4693b180-9160-4daf-9a02-5a64f26d6e45</td>\n      <td>defc36fc-661e-400c-a364-03610f90bd04</td>\n      <td>mpt-chat</td>\n      <td>greedy</td>\n      <td>yes</td>\n      <td>zero_width_space</td>\n      <td>wiki</td>\n      <td>Bart Allen</td>\n      <td>Write the body of a Wikipedia article titled \"...</td>\n      <td>B​a​r​t​ ​A​l​l​e​n​ ​i​s​ ​a​ ​f​i​c​t​i​o​n​...</td>\n    </tr>\n    <tr>\n      <th>5615819</th>\n      <td>56426ad1-2fce-4a55-bd4f-a546520a62ff</td>\n      <td>7be1a55e-694d-47d1-9e9d-e2cfeab8bc36</td>\n      <td>defc36fc-661e-400c-a364-03610f90bd04</td>\n      <td>mpt-chat</td>\n      <td>sampling</td>\n      <td>no</td>\n      <td>zero_width_space</td>\n      <td>wiki</td>\n      <td>Bart Allen</td>\n      <td>Write the body of a Wikipedia article titled \"...</td>\n      <td>B​a​r​t​ ​A​l​l​e​n​ ​i​s​ ​a​ ​f​i​c​t​i​o​n​...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5615820 rows × 11 columns</p>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# print(df['attack'].value_counts())\n# print(df['model'].value_counts())\n# print(df['domain'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'df' is your DataFrame containing the data\n\n# Create the crosstab (pivot table) with 'domain' as rows and 'model' as columns\npivot_table = pd.crosstab(df['domain'], df['model'])\n\n# Display the resulting pivot table\nprint(pivot_table)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-14T10:44:13.908600Z","iopub.execute_input":"2024-11-14T10:44:13.909061Z","iopub.status.idle":"2024-11-14T10:44:15.296001Z","shell.execute_reply.started":"2024-11-14T10:44:13.909014Z","shell.execute_reply":"2024-11-14T10:44:15.294579Z"},"trusted":true},"outputs":[{"name":"stdout","text":"model      chatgpt  cohere  cohere-chat   gpt2   gpt3   gpt4  human  \\\ndomain                                                                \nabstracts    42384   42384        42384  84768  42384  42384  21192   \nbooks        42744   42744        42744  85488  42744  42744  21372   \nnews         42720   42720        42720  85440  42720  42720  21360   \npoetry       42504   42504        42504  85008  42504  42504  21252   \nrecipes      42528   42528        42528  85056  42528  42528  21264   \nreddit       42696   42696        42696  85392  42696  42696  21348   \nreviews      22632   22632        22632  45264  22632  22632  11316   \nwiki         42696   42696        42696  85392  42696  42696  21348   \n\nmodel      llama-chat  mistral  mistral-chat    mpt  mpt-chat  \ndomain                                                         \nabstracts       84768    84768         84768  84768     84768  \nbooks           85488    85488         85488  85488     85488  \nnews            85440    85440         85440  85440     85440  \npoetry          85008    85008         85008  85008     85008  \nrecipes         85056    85056         85056  85056     85056  \nreddit          85392    85392         85392  85392     85392  \nreviews         45264    45264         45264  45264     45264  \nwiki            85392    85392         85392  85392     85392  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"s","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a variable to store processed results (if needed)\nprocessed_data = []\n\n# Iterate through each chunk\nfor chunk in chunk_iter:\n    # Process each chunk (for example, print unique values in the 'domain' column)\n    unique_values = chunk['model'].value_counts()\n    print(unique_values)\n    \n    # If you want to store the processed data, you can append it to a list\n    processed_data.append(chunk)\n\n# If you want to combine all chunks into a single DataFrame at the end, you can use:\n# df = pd.concat(processed_data, ignore_index=True)\nprocessed_data","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a variable to store processed results (if needed)\nprocessed_data = []\n\n# Iterate through each chunk\nfor chunk in chunk_iter:\n    # Process each chunk (for example, print unique values in the 'domain' column)\n    unique_values = chunk['attack'].value_counts()\n    print(unique_values)\n    \n    # If you want to store the processed data, you can append it to a list\n    processed_data.append(chunk)\n\n# If you want to combine all chunks into a single DataFrame at the end, you can use:\n# df = pd.concat(processed_data, ignore_index=True)\nprocessed_data","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reduced_val = 1\n# reduced_val = 0.00001\n# Sample 10% of the DataFrame\ndf = df.sample(n=int(len(df) * reduced_val), random_state=42)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:40:13.206926Z","iopub.execute_input":"2024-11-02T05:40:13.207244Z","iopub.status.idle":"2024-11-02T05:40:13.330805Z","shell.execute_reply.started":"2024-11-02T05:40:13.207211Z","shell.execute_reply":"2024-11-02T05:40:13.329853Z"},"trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"                                          id  \\\n405948  6ea53940-427e-4315-8d93-00782d9b3009   \n165498  30e378ee-fe2a-44f2-a007-f2b19e7521e9   \n126586  8c2cc761-f250-4301-a3e6-c781b8a242cb   \n358729  dae0fc09-cd22-4b4a-ab75-4c575cd14e52   \n127999  307ff426-6a16-493f-8d4c-667a1db14e8e   \n...                                      ...   \n259178  3d6223f4-1ce1-4268-826d-4e7ccbaf23e5   \n365838  7eb153af-63a4-4803-8484-3a9584dccaf5   \n131932  3e8b9568-21b3-4edc-8e32-b773d66a2e03   \n671155  5330aa4c-4d30-4311-939b-8e82707e75ed   \n121958  808ee701-5df7-4edc-a829-4774bc4d0f90   \n\n                                               generation  \n405948  \\n\\nIngredients:\\n\\n1 tablespoon olive oil\\n\\n...  \n165498  \\n​\\n​E​m​p​i​r​e​ ​o​f​ ​t​h​e​ ​S​u​n​ ​i​s​...  \n126586  In 1844, a young boy named William Thomas Smit...  \n358729  1/2 роund udоn nооdlеs (Jараnеsе thісk whеаt-f...  \n127999  ''Skeletons at the Feast' is a novel written b...  \n...                                                   ...  \n259178  How a little girl danced\\n\\nHow a little girl ...  \n365838  Roasted Garlic Goat Cheese and Sesame\\n\\nIngre...  \n131932  On Writing: A Memoir of the Craft is a memoir ...  \n671155  P​a​u​l​ ​W​i​l​h​e​l​m​ ​B​a​l​t​e​s​ ​(​N​o​...  \n121958  In this story, a young man named Fox Mask has ...  \n\n[672000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>generation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>405948</th>\n      <td>6ea53940-427e-4315-8d93-00782d9b3009</td>\n      <td>\\n\\nIngredients:\\n\\n1 tablespoon olive oil\\n\\n...</td>\n    </tr>\n    <tr>\n      <th>165498</th>\n      <td>30e378ee-fe2a-44f2-a007-f2b19e7521e9</td>\n      <td>\\n​\\n​E​m​p​i​r​e​ ​o​f​ ​t​h​e​ ​S​u​n​ ​i​s​...</td>\n    </tr>\n    <tr>\n      <th>126586</th>\n      <td>8c2cc761-f250-4301-a3e6-c781b8a242cb</td>\n      <td>In 1844, a young boy named William Thomas Smit...</td>\n    </tr>\n    <tr>\n      <th>358729</th>\n      <td>dae0fc09-cd22-4b4a-ab75-4c575cd14e52</td>\n      <td>1/2 роund udоn nооdlеs (Jараnеsе thісk whеаt-f...</td>\n    </tr>\n    <tr>\n      <th>127999</th>\n      <td>307ff426-6a16-493f-8d4c-667a1db14e8e</td>\n      <td>''Skeletons at the Feast' is a novel written b...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>259178</th>\n      <td>3d6223f4-1ce1-4268-826d-4e7ccbaf23e5</td>\n      <td>How a little girl danced\\n\\nHow a little girl ...</td>\n    </tr>\n    <tr>\n      <th>365838</th>\n      <td>7eb153af-63a4-4803-8484-3a9584dccaf5</td>\n      <td>Roasted Garlic Goat Cheese and Sesame\\n\\nIngre...</td>\n    </tr>\n    <tr>\n      <th>131932</th>\n      <td>3e8b9568-21b3-4edc-8e32-b773d66a2e03</td>\n      <td>On Writing: A Memoir of the Craft is a memoir ...</td>\n    </tr>\n    <tr>\n      <th>671155</th>\n      <td>5330aa4c-4d30-4311-939b-8e82707e75ed</td>\n      <td>P​a​u​l​ ​W​i​l​h​e​l​m​ ​B​a​l​t​e​s​ ​(​N​o​...</td>\n    </tr>\n    <tr>\n      <th>121958</th>\n      <td>808ee701-5df7-4edc-a829-4774bc4d0f90</td>\n      <td>In this story, a young man named Fox Mask has ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>672000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"import os\nfrom glob import glob\n\nimport torch\nimport wget\nfrom tqdm import tqdm\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\n\nclass GPT2Detector:\n\n    path_to_weights = os.path.abspath(\"detectors/gpt2_detector/openai-gpt-2-detector/detector_weights\")\n    if not os.path.exists(path_to_weights):\n        os.makedirs(path_to_weights)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def __init__(self, model) -> None:\n        self.model_name = model\n        self._check_weights()\n        self._init_model()\n\n    def _check_weights(self):\n        \"\"\"\n        Check if the weights for the given model are present in the current directory.\n        If not, download them from the repository.\n        \"\"\"\n        if self.model_name == \"gpt2-base\":\n            weights_name = \"detector-base.pt\"\n            if not glob(os.path.join(self.path_to_weights, weights_name)):\n                print(\"Downloading weights for GPT2 Base Detector...\")\n                wget.download(\n                    \"https://openaipublic.azureedge.net/gpt-2/detector-models/v1/detector-base.pt\",\n                    out=os.path.join(self.path_to_weights, weights_name),\n                )\n\n        elif self.model_name == \"gpt2-large\":\n            weights_name = \"detector-large.pt\"\n            if not glob(os.path.join(self.path_to_weights, weights_name)):\n                print(\"Downloading weights for GPT2 Large Detector...\")\n                wget.download(\n                    \"https://openaipublic.azureedge.net/gpt-2/detector-models/v1/detector-large.pt\",\n                    out=os.path.join(self.path_to_weights, weights_name),\n                )\n\n    def _init_model(self):\n        model_name = \"roberta-large\" if self.model_name == \"gpt2-large\" else \"roberta-base\"\n        self.model = RobertaForSequenceClassification.from_pretrained(model_name)\n        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n\n        if self.model_name == \"gpt2-base\":\n            weight_name = \"detector-base.pt\"\n        else:\n            weight_name = \"detector-large.pt\"\n\n        checkpoint = os.path.join(self.path_to_weights, weight_name)\n        data = torch.load(checkpoint, map_location=\"cpu\")\n\n        self.model.load_state_dict(data[\"model_state_dict\"], strict=False)\n        self.model.eval()\n\n    def inference(self, texts: list) -> list:\n        predictions = []\n        for text in tqdm(texts):\n            tokens = self.tokenizer.encode(text)\n            tokens = tokens[: self.tokenizer.model_max_length - 2]\n            tokens = torch.tensor([self.tokenizer.bos_token_id] + tokens + [self.tokenizer.eos_token_id]).unsqueeze(0)\n            mask = torch.ones_like(tokens)\n\n            self.model = self.model.to(self.device)\n            with torch.no_grad():\n                logits = self.model(tokens.to(self.device), attention_mask=mask.to(self.device))[0]\n                probs = logits.softmax(dim=-1)\n\n            fake, real = probs.detach().cpu().flatten().numpy().tolist()\n            print('fake ',fake, ' and real ',real)\n            predictions.append(fake)\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:37:12.902683Z","iopub.execute_input":"2024-11-02T05:37:12.903033Z","iopub.status.idle":"2024-11-02T05:37:20.900181Z","shell.execute_reply.started":"2024-11-02T05:37:12.902999Z","shell.execute_reply":"2024-11-02T05:37:20.899196Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q wandb==0.16.6","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-02T05:37:20.903617Z","iopub.execute_input":"2024-11-02T05:37:20.904194Z","iopub.status.idle":"2024-11-02T05:38:28.760781Z","shell.execute_reply.started":"2024-11-02T05:37:20.904151Z","shell.execute_reply":"2024-11-02T05:38:28.759476Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# # Initialize Weights & Biases (W&B) in disabled mode.\n\nimport wandb\nwandb.init(mode=\"disabled\")","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:28.762487Z","iopub.execute_input":"2024-11-02T05:38:28.762921Z","iopub.status.idle":"2024-11-02T05:38:30.763290Z","shell.execute_reply.started":"2024-11-02T05:38:28.762873Z","shell.execute_reply":"2024-11-02T05:38:30.762443Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n\nclass detector_model():\n    def __init__(self, model):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model).to(\n            self.device\n        )\n\n    def inference(self, texts: list) -> list:\n        predictions = []\n        for text in tqdm(texts):\n            inputs = self.tokenizer(text, truncation=True, return_tensors=\"pt\").to(self.device)\n            outputs = self.model(**inputs)\n            probs = outputs.logits.softmax(dim=-1)\n            real, fake = probs.detach().cpu().flatten().numpy().tolist()\n            #print('fake ',fake, ' and real ',real)\n\n            predictions.append(fake)\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:30.764568Z","iopub.execute_input":"2024-11-02T05:38:30.765057Z","iopub.status.idle":"2024-11-02T05:38:30.791721Z","shell.execute_reply.started":"2024-11-02T05:38:30.765022Z","shell.execute_reply":"2024-11-02T05:38:30.790927Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model_name = '/kaggle/input/eng-ensemble-coling-final/eng_roberta-base_6eps'\n\ndetector = detector_model(model_name)\ndetector","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:30.792805Z","iopub.execute_input":"2024-11-02T05:38:30.793104Z","iopub.status.idle":"2024-11-02T05:38:36.126484Z","shell.execute_reply.started":"2024-11-02T05:38:30.793072Z","shell.execute_reply":"2024-11-02T05:38:36.125217Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<__main__.detector_model at 0x7ab9afab2b90>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def run_detection_custom(f, df):\n    # Make a copy of the IDs of the original dataframe to avoid editing in place\n    scores_df = df[[\"id\"]].copy()\n\n    # Run the detector function on the dataset and put output in score column\n    scores_df[\"score\"] = f(df[\"generation\"].tolist())\n\n    # Convert scores and ids to dict in 'records' format for seralization\n    # e.g. [{'id':'...', 'score':0}, {'id':'...', 'score':1}, ...]\n    results = scores_df[[\"id\", \"score\"]].to_dict(orient=\"records\")\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:36.127684Z","iopub.execute_input":"2024-11-02T05:38:36.128121Z","iopub.status.idle":"2024-11-02T05:38:36.134869Z","shell.execute_reply.started":"2024-11-02T05:38:36.128075Z","shell.execute_reply":"2024-11-02T05:38:36.133870Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(f\"Running detection...\")\nresults = run_detection_custom(detector.inference, df)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n\nclass Radar:\n    def __init__(self):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.detector = AutoModelForSequenceClassification.from_pretrained(\n            \"TrustSafeAI/RADAR-Vicuna-7B\", cache_dir=os.environ[\"CACHE_DIR\"]\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(\"TrustSafeAI/RADAR-Vicuna-7B\")\n        self.detector.eval()\n        self.detector.to(self.device)\n\n    def inference(self, texts: list) -> list:\n        predictions = []\n        for text in tqdm(texts):\n            with torch.no_grad():\n                inputs = self.tokenizer([text], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n                output_probs = F.log_softmax(self.detector(**inputs).logits, -1)[:, 0].exp().tolist()\n                print('outputs ', output_probs)\n            predictions.append(output_probs[0])\n        return predictions\n\n# radar = Radar()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:36.925729Z","iopub.execute_input":"2024-11-02T05:38:36.926015Z","iopub.status.idle":"2024-11-02T05:38:36.935841Z","shell.execute_reply.started":"2024-11-02T05:38:36.925984Z","shell.execute_reply":"2024-11-02T05:38:36.935013Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"len(results)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-11-02T05:38:36.936896Z","iopub.execute_input":"2024-11-02T05:38:36.937291Z","iopub.status.idle":"2024-11-02T05:38:36.953477Z","shell.execute_reply.started":"2024-11-02T05:38:36.937232Z","shell.execute_reply":"2024-11-02T05:38:36.952570Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import json\ntem_name = model_name.split('/')[-1]\noutput_path = f'/kaggle/working/pred_file_{tem_name}.json'\nwith open(output_path, \"w\") as f:\n        json.dump(results, f)\nprint('saved to path: ',output_path)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:36.954760Z","iopub.execute_input":"2024-11-02T05:38:36.955088Z","iopub.status.idle":"2024-11-02T05:38:36.962853Z","shell.execute_reply.started":"2024-11-02T05:38:36.955042Z","shell.execute_reply":"2024-11-02T05:38:36.961948Z"},"trusted":true},"outputs":[{"name":"stdout","text":"saved to path:  /kaggle/working/pred_file_eng_roberta-base_6eps.json\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# from raid import run_detection, run_evaluation\nprint(f\"Running evaluation...\")\n\n# Evaluate your detector predictions\n# evaluation_result = run_evaluation(results, df)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T05:38:36.967173Z","iopub.execute_input":"2024-11-02T05:38:36.967512Z","iopub.status.idle":"2024-11-02T05:38:36.972718Z","shell.execute_reply.started":"2024-11-02T05:38:36.967473Z","shell.execute_reply":"2024-11-02T05:38:36.971763Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running evaluation...\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}